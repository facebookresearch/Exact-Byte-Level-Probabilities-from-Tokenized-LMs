# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from typing import Optional
import torch
from byte_prediction_model import BytePredLLM
from string_processor.string_helper import SamplerState
from byte_gen_utils import (
    sample_top_p,
    temp_scale_prob,
)


class ByteEnsemble:
    models: list[BytePredLLM]

    def __init__(self, paths: list[str], device="gpu"):
        models = []
        import os

        if device == "gpu":
            start_idx = 0
            gpu_ids = [start_idx + i for i in range(len(paths))]
            visible_gpus = torch.cuda.device_count()
            print(
                f"visible gpus: {visible_gpus}, using following gpus for ensemble: {gpu_ids}"
            )
            for gpu_id, path in zip(gpu_ids, paths):
                models.append(BytePredLLM(path, device=f"cuda:{gpu_id}"))
        else:
            for path in paths:
                models.append(BytePredLLM(path, device=f"cpu"))
        self.models = models

        self.white_space = self.models[0].t_wrapper.white_space
        for model in self.models:
            assert model.t_wrapper.white_space == self.models[0].t_wrapper.white_space

    def prob_next_byte(
        self,
        raw_utf8_seq,
        fast_mode=False,
        sampler_state_list: Optional[SamplerState] = None,
        last_byte: Optional[int] = None,
    ):

        if sampler_state_list is None:
            pred_list = []
            sampler_state_list = []
            for model in self.models:
                pred, sampler_state = model.prob_next_byte(
                    raw_utf8_seq=raw_utf8_seq, fast_mode=fast_mode
                )
                pred_list.append((pred[:259] / pred[:259].sum()).cpu())
                sampler_state_list.append(sampler_state)
        else:
            pred_list = [None] * len(self.models)
            for i, model in enumerate(self.models):
                pred, sampler_state = model.prob_next_byte(
                    raw_utf8_seq=raw_utf8_seq,
                    fast_mode=fast_mode,
                    sampler_state=sampler_state_list[i],
                    last_byte=last_byte,
                )
                pred_list[i] = (pred[:259] / pred[:259].sum()).cpu()
                sampler_state_list[i] = sampler_state

        merged_pred = torch.stack(pred_list)
        merged_pred = merged_pred.mean(dim=0)

        return merged_pred, sampler_state_list

    def generate(
        self,
        input_str: str,
        fast_mode=False,
        temperature=1.0,
        top_p=1.0,
        max_new_bytes: int = 100,
    ):
        """Main call for the wrapper, API similar to [1].

        [1] https://github.com/huggingface/transformers/blob/v4.44.0/src/transformers/generation/configuration_utils.py#L71

        Args:
            input_str (str): unprocessed string. must end with a valid utf-8 characters.
            fast_mode (bool): whether to truncate invalid encodings (False) or not (True). This is for speed-up generations.
            temperature (float): temperature to scale probability.
            top_p (float): top_p value for predictions.
            max_new_tokens (int): number to bytes to generate.

        Returns:
            inp_bytes: output generated bytes.
        """

        # input string --> utf-8 bytes
        input_str = input_str.replace(" ", self.white_space)
        inp_bytes = list(input_str.encode("utf-8"))
        sampler_state_list = None
        last_byte = None

        for _ in range(max_new_bytes):
            predictions, sampler_state_list = self.prob_next_byte(
                inp_bytes,
                fast_mode=fast_mode,
                sampler_state_list=sampler_state_list,
                last_byte=last_byte,
            )

            if temperature < 0.1:
                last_byte = predictions.argmax().item()
            else:
                predictions = temp_scale_prob(predictions, temperature)
                last_byte = sample_top_p(predictions, top_p, return_probs=False).item()

            inp_bytes = inp_bytes + [last_byte]
            try:
                print(bytes(inp_bytes).decode("utf-8"))
            except:
                pass

        return inp_bytes
